{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "#load datasets\n",
    "with open('chinese_stopwords.txt','r',encoding='utf-8') as file:\n",
    "    stopwords=[i[:-1]for i in file.readlines()]\n",
    "    \n",
    "news=pd.read_csv('sqlResult.csv',encoding='gb18030')\n",
    "print(news.shape)\n",
    "print(news.head())\n",
    "#preprocessing\n",
    "print(news[news.content.isna()].head())\n",
    "news=news.dropna(subset=['content'])\n",
    "print(news.shape)\n",
    "\n",
    "def split_text(text):\n",
    "    text=text.replace('','')\n",
    "    text=text.replace('\\n','')\n",
    "    text2=jieba.cut(text.strip())\n",
    "    result=''.join([w for w in text2 if w not in stopwords])\n",
    "    return result\n",
    "print (news.iloc[0].content)\n",
    "print(split_text(news.iloc[0].content))\n",
    "\n",
    "import pickle,os\n",
    "if not os.path.exists('corpus.pkl'):\n",
    "    corpus=list(map(split_text,[str(i)for i in news.content]))\n",
    "    print (corpus[0])\n",
    "    print(len(corpus))\n",
    "    with open('corpus.pkl','wb') as file:\n",
    "        pickle.dump(corpus,file)\n",
    "else:\n",
    "    with open('corpus.pkl','rb') as file:\n",
    "        corpus=pickle.load(file)\n",
    "\n",
    "countvectorizer=CountVectorizer(encoding='gb18030', min_df=0.015)\n",
    "tfidftransformer=TfidfTransformer()\n",
    "countervector=countvectorizer.fit_transform(corpus)\n",
    "tfidf=tfidftransformer.fit_transform(countervector)\n",
    "print(tfidf.shape)\n",
    "\n",
    "label=list(map(lambda source:1 if 'xinhua' in str(source) else 0,news.source))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model.selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "x_train, x_test, y_train, y_test=train_test_split(tfidf).toarray(),label,test_size=0.3, random_state=33)\n",
    "clf=MultinomialNB()\n",
    "clf.fit(x_train,y_train )\n",
    "prediction=clf.predict(tfidf.toarray())\n",
    "labels=np.array(label)\n",
    "compare_news_index=pd.DataFrame({'prediction':prediction,'labels':labels})\n",
    "copy_news_index=compare_news_index[(compare_news_index['prediction']==1)&(compare_news_inedx['labels']==0)].index\n",
    "xinhuashe_news_index=compare_news_index[(compare_news_index['labels']==1)].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer=Normalizer()\n",
    "scaled_array=normalizer.fit_transform(tfidf.toarray())\n",
    "if not os.path.exists('label.pkl'):\n",
    "    kmeans=Kmeans(n_clusters=25)\n",
    "    k_labels=kmeans.fit_predict(scaled_array)\n",
    "    with open('label.pkl','wb') as file:\n",
    "        pickle.dump(k_labels, file)\n",
    "    print('k_labels.shape',k_labels.shape)\n",
    "else:\n",
    "    with open('label.pkl','rb') as file:\n",
    "        k_labels=pickle.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('id_class.pkl'): \n",
    "    id_class={index:class_ for index, class_ in enumerate(k_labels)}\n",
    "    with open('id_class.pkl','wb') as file:\n",
    "        pickle.dump(id_class, file)\n",
    "else:\n",
    "    with open('id_class.pkl','rb') as file:\n",
    "        id_class=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('class_id.pkl'):\n",
    "    from collections import defaultdict\n",
    "    class_id=defaultdict(set)\n",
    "    for index, class_ in id_class.items():\n",
    "        if index in xinhuashe_news_index.tolist():\n",
    "            class_id[class_].add(index)\n",
    "    with open('class_id.pkl','wb') as file:\n",
    "        pickle.dump(class_id, file)\n",
    "else:\n",
    "    with open('class_id.pkl','rb') as file:\n",
    "        id_class=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_text(cpindex,top=10): dist_dict={i:cosine_similarity(tfiddf[cpindex],tfidf[i]) for i in class_id[id_class[cpindex]]}\n",
    "return sorted(dist_dict.items(),key=lambda x:x[1][0], reverse=True)[:top]\n",
    "cpindex=3352 \n",
    "similar_list=find_similar_text(cpindex) \n",
    "similar2=similar_list[0][0]\n",
    "print('similar content\\n',news.iloc[similar2].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
