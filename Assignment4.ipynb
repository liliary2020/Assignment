{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow.contrib.keras as kr\n",
    "import torch.utils.data as Data\n",
    "\n",
    "def get_labels():\n",
    "    \"\"\"\n",
    "    获取所有类别标签，并转换成id字典形式\n",
    "    \"\"\"\n",
    "    labels = ['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
    "    labels_dict = dict(zip(labels, range(len(labels))))\n",
    "    return labels, labels_dict\n",
    "\n",
    "def get_vocab(filename):\n",
    "    \"\"\"\n",
    "    获取词表，并转换成ID字典形式\n",
    "    \"\"\"\n",
    "    with open(filename, encoding='utf-8', errors='ignore') as file:\n",
    "        word = [line.strip() for line in file.readlines()]\n",
    "        word_dict = dict(zip(word, range(len(word))))\n",
    "        \n",
    "    return word, word_dict\n",
    "\n",
    "def process_file(filename, word_dict, label_dict, maxlen):\n",
    "    \"\"\"\n",
    "    从文件中获取数据并转换成id形式\n",
    "    \"\"\"\n",
    "    context, labels = [],[]\n",
    "    with open(filename, encoding='utf-8', errors='ignore') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                label, content = line.strip().split('\\t')\n",
    "                if content:\n",
    "                    context.append(list(content))\n",
    "                    labels.append(label)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    data_id, label_id = [], []\n",
    "    for i in range(len(context)):\n",
    "        data_id.append([word_dict[x] for x in context[i] if x in word_dict])#将每句话id化\n",
    "        label_id.append(label_dict[labels[i]])#每句话对应的类别的id\n",
    "\n",
    "    # # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, maxlen)\n",
    "    y_pad = kr.utils.to_categorical(label_id, num_classes=len(label_dict))  # 将标签转换为one-hot表示\n",
    "    return x_pad, y_pad\n",
    "\n",
    "label, label_dict = get_labels()\n",
    "word, word_dict = get_vocab('cnews.vocab.txt')\n",
    "\n",
    "x_train, y_train = process_file('cnews.train.txt', word_dict, label_dict, 600)\n",
    "x_val, y_val = process_file('cnews.val.txt', word_dict, label_dict, 600)\n",
    "x_test, y_test = process_file('cnews.test.txt', word_dict, label_dict, 600)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "x_train, y_train = torch.LongTensor(x_train), torch.Tensor(y_train)\n",
    "x_val, y_val = torch.LongTensor(x_val), torch.Tensor(y_val)\n",
    "\n",
    "# train_dataset = Data.TensorDataset(x_train, y_train) \n",
    "# train_dataloader = Data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# val_dataset = Data.TensorDataset(x_val, y_val)\n",
    "# val_dataloader = Data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1)\n",
    "\n",
    "\n",
    "def batch_iter(x, y, batch_size=64):\n",
    "    \"\"\"生成批次数据\"\"\"\n",
    "    data_len = len(x)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    " \n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuffle = x[indices]\n",
    "    y_shuffle = y[indices]\n",
    " \n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,step:1,acc_train:0.0,loss_train:2.317220687866211\n",
      "epoch:0,step:101,acc_train:0.34375,loss_train:1.9952791929244995\n",
      "epoch:0,step:201,acc_train:0.3125,loss_train:1.7078511714935303\n",
      "epoch:0,step:301,acc_train:0.46875,loss_train:1.44394052028656\n",
      "epoch:0,step:401,acc_train:0.71875,loss_train:0.8115702271461487\n",
      "epoch:0,step:501,acc_train:0.65625,loss_train:0.783383309841156\n",
      "epoch:0,step:601,acc_train:0.8125,loss_train:0.6549707055091858\n",
      "epoch:0,step:701,acc_train:0.9375,loss_train:0.36371129751205444\n",
      "epoch:0,step:801,acc_train:0.875,loss_train:0.4315113127231598\n",
      "epoch:0,step:901,acc_train:0.8125,loss_train:0.6004424691200256\n",
      "epoch:0,step:1001,acc_train:0.75,loss_train:0.698325514793396\n",
      "epoch:0,step:1101,acc_train:0.84375,loss_train:0.6217854022979736\n",
      "epoch:0,step:1201,acc_train:0.65625,loss_train:0.9829463362693787\n",
      "epoch:0,step:1301,acc_train:0.78125,loss_train:0.5777592062950134\n",
      "epoch:0,step:1401,acc_train:0.875,loss_train:0.23794180154800415\n",
      "epoch:0,step:1501,acc_train:0.90625,loss_train:0.20128291845321655\n",
      "epoch:0,step:1563,acc_val:0.8459394904458599\n",
      "epoch:1,step:1564,acc_train:0.75,loss_train:0.6517558097839355\n",
      "epoch:1,step:1664,acc_train:0.84375,loss_train:0.37147605419158936\n",
      "epoch:1,step:1764,acc_train:0.96875,loss_train:0.1567450761795044\n",
      "epoch:1,step:1864,acc_train:0.8125,loss_train:0.6471871733665466\n",
      "epoch:1,step:1964,acc_train:0.90625,loss_train:0.21275973320007324\n",
      "epoch:1,step:2064,acc_train:0.875,loss_train:0.4049345850944519\n",
      "epoch:1,step:2164,acc_train:0.90625,loss_train:0.3358108401298523\n",
      "epoch:1,step:2264,acc_train:0.84375,loss_train:0.5633550882339478\n",
      "epoch:1,step:2364,acc_train:0.90625,loss_train:0.3440386950969696\n",
      "epoch:1,step:2464,acc_train:0.90625,loss_train:0.3108401596546173\n",
      "epoch:1,step:2564,acc_train:0.90625,loss_train:0.28636637330055237\n",
      "epoch:1,step:2664,acc_train:0.75,loss_train:0.5112646818161011\n",
      "epoch:1,step:2764,acc_train:0.9375,loss_train:0.1847929060459137\n",
      "epoch:1,step:2864,acc_train:0.875,loss_train:0.41173771023750305\n",
      "epoch:1,step:2964,acc_train:0.84375,loss_train:0.6178615093231201\n",
      "epoch:1,step:3064,acc_train:0.84375,loss_train:0.5001471638679504\n",
      "epoch:1,step:3126,acc_val:0.8855493630573248\n",
      "epoch:2,step:3127,acc_train:0.75,loss_train:0.6281901597976685\n",
      "epoch:2,step:3227,acc_train:0.9375,loss_train:0.20379823446273804\n",
      "epoch:2,step:3327,acc_train:0.9375,loss_train:0.20309732854366302\n",
      "epoch:2,step:3427,acc_train:0.84375,loss_train:0.6158146858215332\n",
      "epoch:2,step:3527,acc_train:0.84375,loss_train:0.5706000924110413\n",
      "epoch:2,step:3627,acc_train:0.9375,loss_train:0.22574955224990845\n",
      "epoch:2,step:3727,acc_train:0.875,loss_train:0.3300900161266327\n",
      "epoch:2,step:3827,acc_train:0.875,loss_train:0.25494566559791565\n",
      "epoch:2,step:3927,acc_train:0.84375,loss_train:0.33699265122413635\n",
      "epoch:2,step:4027,acc_train:0.84375,loss_train:0.4853876829147339\n",
      "epoch:2,step:4127,acc_train:0.84375,loss_train:0.43967771530151367\n",
      "epoch:2,step:4227,acc_train:0.875,loss_train:0.2674012780189514\n",
      "epoch:2,step:4327,acc_train:0.84375,loss_train:0.40613266825675964\n",
      "epoch:2,step:4427,acc_train:0.9375,loss_train:0.3583851754665375\n",
      "epoch:2,step:4527,acc_train:0.90625,loss_train:0.28584587574005127\n",
      "epoch:2,step:4627,acc_train:0.96875,loss_train:0.17320482432842255\n",
      "epoch:2,step:4689,acc_val:0.8811703821656051\n",
      "epoch:3,step:4690,acc_train:0.875,loss_train:0.32198989391326904\n",
      "epoch:3,step:4790,acc_train:1.0,loss_train:0.052317872643470764\n",
      "epoch:3,step:4890,acc_train:0.9375,loss_train:0.1939384490251541\n",
      "epoch:3,step:4990,acc_train:0.96875,loss_train:0.11734528839588165\n",
      "epoch:3,step:5090,acc_train:0.90625,loss_train:0.1415398269891739\n",
      "epoch:3,step:5190,acc_train:0.90625,loss_train:0.29711443185806274\n",
      "epoch:3,step:5290,acc_train:0.90625,loss_train:0.162973552942276\n",
      "epoch:3,step:5390,acc_train:0.875,loss_train:0.4582720100879669\n",
      "epoch:3,step:5490,acc_train:0.90625,loss_train:0.3690420389175415\n",
      "epoch:3,step:5590,acc_train:0.90625,loss_train:0.22964900732040405\n",
      "epoch:3,step:5690,acc_train:0.96875,loss_train:0.055837661027908325\n",
      "epoch:3,step:5790,acc_train:0.8125,loss_train:0.4589512348175049\n",
      "epoch:3,step:5890,acc_train:0.96875,loss_train:0.07716726511716843\n",
      "epoch:3,step:5990,acc_train:0.875,loss_train:0.276631236076355\n",
      "epoch:3,step:6090,acc_train:0.96875,loss_train:0.08944639563560486\n",
      "epoch:3,step:6190,acc_train:0.90625,loss_train:0.24501095712184906\n",
      "epoch:3,step:6252,acc_val:0.9066480891719745\n",
      "epoch:4,step:6253,acc_train:0.84375,loss_train:0.3893071115016937\n",
      "epoch:4,step:6353,acc_train:0.90625,loss_train:0.4046012759208679\n",
      "epoch:4,step:6453,acc_train:0.875,loss_train:0.32419437170028687\n",
      "epoch:4,step:6553,acc_train:0.90625,loss_train:0.36667972803115845\n",
      "epoch:4,step:6653,acc_train:0.84375,loss_train:0.3474580645561218\n",
      "epoch:4,step:6753,acc_train:0.84375,loss_train:0.4732326567173004\n",
      "epoch:4,step:6853,acc_train:0.90625,loss_train:0.18089599907398224\n",
      "epoch:4,step:6953,acc_train:0.96875,loss_train:0.14279158413410187\n",
      "epoch:4,step:7053,acc_train:1.0,loss_train:0.009421974420547485\n",
      "epoch:4,step:7153,acc_train:0.9375,loss_train:0.13897176086902618\n",
      "epoch:4,step:7253,acc_train:0.96875,loss_train:0.1491701900959015\n",
      "epoch:4,step:7353,acc_train:0.875,loss_train:0.36201775074005127\n",
      "epoch:4,step:7453,acc_train:0.96875,loss_train:0.18973547220230103\n",
      "epoch:4,step:7553,acc_train:0.875,loss_train:0.21104058623313904\n",
      "epoch:4,step:7653,acc_train:0.90625,loss_train:0.233294278383255\n",
      "epoch:4,step:7753,acc_train:0.96875,loss_train:0.1571415364742279\n",
      "epoch:4,step:7815,acc_val:0.8958996815286624\n",
      "epoch:5,step:7816,acc_train:0.90625,loss_train:0.14513687789440155\n",
      "epoch:5,step:7916,acc_train:0.9375,loss_train:0.2201595902442932\n",
      "epoch:5,step:8016,acc_train:0.90625,loss_train:0.219037264585495\n",
      "epoch:5,step:8116,acc_train:0.90625,loss_train:0.26693204045295715\n",
      "epoch:5,step:8216,acc_train:0.96875,loss_train:0.05139802768826485\n",
      "epoch:5,step:8316,acc_train:0.96875,loss_train:0.09323935955762863\n",
      "epoch:5,step:8416,acc_train:1.0,loss_train:0.07803407311439514\n",
      "epoch:5,step:8516,acc_train:0.90625,loss_train:0.45761600136756897\n",
      "epoch:5,step:8616,acc_train:0.84375,loss_train:0.4423027038574219\n",
      "epoch:5,step:8716,acc_train:0.9375,loss_train:0.22309356927871704\n",
      "epoch:5,step:8816,acc_train:0.96875,loss_train:0.09686406701803207\n",
      "epoch:5,step:8916,acc_train:0.9375,loss_train:0.2175140380859375\n",
      "epoch:5,step:9016,acc_train:0.96875,loss_train:0.06868389993906021\n",
      "epoch:5,step:9116,acc_train:1.0,loss_train:0.06776885688304901\n",
      "epoch:5,step:9216,acc_train:1.0,loss_train:0.04294033348560333\n",
      "epoch:5,step:9316,acc_train:0.9375,loss_train:0.19279271364212036\n",
      "epoch:5,step:9378,acc_val:0.8935111464968153\n",
      "epoch:6,step:9379,acc_train:1.0,loss_train:0.0527188703417778\n",
      "epoch:6,step:9479,acc_train:0.9375,loss_train:0.1790831983089447\n",
      "epoch:6,step:9579,acc_train:1.0,loss_train:0.01271677017211914\n",
      "epoch:6,step:9679,acc_train:0.9375,loss_train:0.1839807778596878\n",
      "epoch:6,step:9779,acc_train:0.96875,loss_train:0.1192941665649414\n",
      "epoch:6,step:9879,acc_train:0.9375,loss_train:0.27344685792922974\n",
      "epoch:6,step:9979,acc_train:0.90625,loss_train:0.34340062737464905\n",
      "epoch:6,step:10079,acc_train:0.96875,loss_train:0.10213948041200638\n",
      "epoch:6,step:10179,acc_train:1.0,loss_train:0.05146370828151703\n",
      "epoch:6,step:10279,acc_train:0.96875,loss_train:0.05285654962062836\n",
      "epoch:6,step:10379,acc_train:0.9375,loss_train:0.19150537252426147\n",
      "epoch:6,step:10479,acc_train:0.875,loss_train:0.21617776155471802\n",
      "epoch:6,step:10579,acc_train:0.9375,loss_train:0.17733895778656006\n",
      "epoch:6,step:10679,acc_train:0.96875,loss_train:0.12954987585544586\n",
      "epoch:6,step:10779,acc_train:0.9375,loss_train:0.15251293778419495\n",
      "epoch:6,step:10879,acc_train:0.90625,loss_train:0.2772105038166046\n",
      "epoch:6,step:10941,acc_val:0.888734076433121\n",
      "epoch:7,step:10942,acc_train:0.96875,loss_train:0.06358757615089417\n",
      "epoch:7,step:11042,acc_train:1.0,loss_train:0.0657883733510971\n",
      "epoch:7,step:11142,acc_train:1.0,loss_train:0.014838159084320068\n",
      "epoch:7,step:11242,acc_train:1.0,loss_train:0.04524771869182587\n",
      "epoch:7,step:11342,acc_train:0.84375,loss_train:0.34982797503471375\n",
      "epoch:7,step:11442,acc_train:0.96875,loss_train:0.15090081095695496\n",
      "epoch:7,step:11542,acc_train:0.96875,loss_train:0.12428224086761475\n",
      "epoch:7,step:11642,acc_train:1.0,loss_train:0.047545067965984344\n",
      "epoch:7,step:11742,acc_train:0.8125,loss_train:0.5240013599395752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7,step:11842,acc_train:0.9375,loss_train:0.1495477259159088\n",
      "epoch:7,step:11942,acc_train:0.96875,loss_train:0.11055899411439896\n",
      "epoch:7,step:12042,acc_train:0.96875,loss_train:0.07442628592252731\n",
      "epoch:7,step:12142,acc_train:0.9375,loss_train:0.14182725548744202\n",
      "epoch:7,step:12242,acc_train:0.9375,loss_train:0.0753379538655281\n",
      "epoch:7,step:12342,acc_train:0.9375,loss_train:0.19853970408439636\n",
      "epoch:7,step:12442,acc_train:1.0,loss_train:0.028598982840776443\n",
      "epoch:7,step:12504,acc_val:0.8960987261146497\n",
      "epoch:8,step:12505,acc_train:0.96875,loss_train:0.10463523864746094\n",
      "epoch:8,step:12605,acc_train:0.90625,loss_train:0.581418514251709\n",
      "epoch:8,step:12705,acc_train:1.0,loss_train:0.053128331899642944\n",
      "epoch:8,step:12805,acc_train:0.9375,loss_train:0.09758573770523071\n",
      "epoch:8,step:12905,acc_train:1.0,loss_train:0.022051610052585602\n",
      "epoch:8,step:13005,acc_train:0.96875,loss_train:0.04223306477069855\n",
      "epoch:8,step:13105,acc_train:0.9375,loss_train:0.13847634196281433\n",
      "epoch:8,step:13205,acc_train:0.9375,loss_train:0.12936855852603912\n",
      "epoch:8,step:13305,acc_train:1.0,loss_train:0.03333001583814621\n",
      "epoch:8,step:13405,acc_train:1.0,loss_train:0.07969368249177933\n",
      "epoch:8,step:13505,acc_train:0.875,loss_train:0.3942347764968872\n",
      "epoch:8,step:13605,acc_train:0.9375,loss_train:0.09788122773170471\n",
      "epoch:8,step:13705,acc_train:0.96875,loss_train:0.21058349311351776\n",
      "epoch:8,step:13805,acc_train:0.96875,loss_train:0.2018166184425354\n",
      "epoch:8,step:13905,acc_train:0.96875,loss_train:0.11547831445932388\n",
      "epoch:8,step:14005,acc_train:0.90625,loss_train:0.4869368076324463\n",
      "epoch:8,step:14067,acc_val:0.8996815286624203\n",
      "epoch:9,step:14068,acc_train:1.0,loss_train:0.0806845873594284\n",
      "epoch:9,step:14168,acc_train:0.96875,loss_train:0.04454624652862549\n",
      "epoch:9,step:14268,acc_train:0.90625,loss_train:0.1390167623758316\n",
      "epoch:9,step:14368,acc_train:1.0,loss_train:0.07341459393501282\n",
      "epoch:9,step:14468,acc_train:1.0,loss_train:0.010223224759101868\n",
      "epoch:9,step:14568,acc_train:0.96875,loss_train:0.15983745455741882\n",
      "epoch:9,step:14668,acc_train:0.96875,loss_train:0.1639668196439743\n",
      "epoch:9,step:14768,acc_train:1.0,loss_train:0.007600635290145874\n",
      "epoch:9,step:14868,acc_train:0.96875,loss_train:0.08551021665334702\n",
      "epoch:9,step:14968,acc_train:1.0,loss_train:0.009438246488571167\n",
      "epoch:9,step:15068,acc_train:0.9375,loss_train:0.19216017425060272\n",
      "epoch:9,step:15168,acc_train:0.96875,loss_train:0.10604958981275558\n",
      "epoch:9,step:15268,acc_train:0.9375,loss_train:0.15517708659172058\n",
      "epoch:9,step:15368,acc_train:0.84375,loss_train:0.49251431226730347\n",
      "epoch:9,step:15468,acc_train:0.96875,loss_train:0.06915195286273956\n",
      "epoch:9,step:15568,acc_train:0.9375,loss_train:0.06373341381549835\n",
      "epoch:9,step:15630,acc_val:0.9000796178343949\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-04.\n",
      "epoch:10,step:15631,acc_train:0.9375,loss_train:0.16620229184627533\n",
      "epoch:10,step:15731,acc_train:1.0,loss_train:0.0025065094232559204\n",
      "epoch:10,step:15831,acc_train:0.96875,loss_train:0.05586206167936325\n",
      "epoch:10,step:15931,acc_train:1.0,loss_train:0.017110690474510193\n",
      "epoch:10,step:16031,acc_train:1.0,loss_train:0.01009562611579895\n",
      "epoch:10,step:16131,acc_train:1.0,loss_train:0.04321056604385376\n",
      "epoch:10,step:16231,acc_train:0.9375,loss_train:0.16539038717746735\n",
      "epoch:10,step:16331,acc_train:0.96875,loss_train:0.11370003968477249\n",
      "epoch:10,step:16431,acc_train:1.0,loss_train:0.017335884273052216\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-69af4c7bf21d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch:{},step:{},acc_val:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_val_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_val_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-69af4c7bf21d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0moptimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0moptimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(5000, 64)\n",
    "        self.rnn = nn.LSTM(input_size=64, hidden_size=128, num_layers=1, bidirectional=False)\n",
    "        self.f1 = nn.Sequential(nn.Linear(128*600, 128,bias=True),\n",
    "                                \n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(0.8)\n",
    "                                )\n",
    "        self.f2 = nn.Sequential(nn.Linear(128, 10,bias=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        \n",
    "        x,_ = self.rnn(x)\n",
    "       \n",
    "        x=x.view(-1,128*600)\n",
    "        \n",
    "        \n",
    "#         x=x[:,-1,:]\n",
    "        x = self.f1(x)\n",
    "        x = self.f2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def train():\n",
    "    model = TextRNN().cuda()\n",
    "    Loss = nn.CrossEntropyLoss()#nn.MultiLabelSoftMarginLoss()\n",
    "    optimer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimer,\n",
    "                                                           mode='max', \n",
    "                                                           patience=5,\n",
    "                                                           verbose=True,\n",
    "                                                           min_lr=1.e-6)\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    \n",
    "    total_iter=0\n",
    "    for epoch in range(100):\n",
    "        \n",
    "        model.train()\n",
    "        for step,(batch_x, batch_y) in enumerate(train_dataloader):\n",
    "            \n",
    "            \n",
    "            \n",
    "            total_iter+=1\n",
    "            x = batch_x.cuda()\n",
    "            y = batch_y.cuda().long()\n",
    "            y_label= torch.argmax(y,1)\n",
    "            \n",
    "            out = model(x)\n",
    "            loss = Loss(out, y_label)\n",
    "            optimer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimer.step()\n",
    "            \n",
    "            preds=torch.softmax(out,dim=1)\n",
    "            acc_train = np.mean((torch.argmax(preds,1) == torch.argmax(y,1)).cpu().numpy())\n",
    "            \n",
    "            if step%100==0:\n",
    "               \n",
    "                print(\"epoch:{},step:{},acc_train:{},loss_train:{}\".format(epoch,total_iter,acc_train,loss))\n",
    "            \n",
    "        acc_val_val=0\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        for step,(batch_x, batch_y) in enumerate(val_dataloader):\n",
    "            \n",
    "            x = batch_x.cuda()\n",
    "            y = batch_y.cuda().long()\n",
    "            y_label= torch.argmax(y,1)\n",
    "            out = model(x)\n",
    "            \n",
    "            preds=torch.softmax(out,dim=1)\n",
    "            acc_val = np.mean((torch.argmax(preds,1) == torch.argmax(y,1)).cpu().numpy())\n",
    "            acc_val_val+=acc_val\n",
    "            if acc_val > best_val_acc:\n",
    "                torch.save(model.state_dict(), './model_params.pkl')\n",
    "                best_val_acc = acc_val\n",
    "        acc_val_=acc_val_val/(step+1)\n",
    "        print(\"epoch:{},step:{},acc_val:{}\".format(epoch,total_iter,acc_val_))\n",
    "        scheduler.step(acc_val_)\n",
    "train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### may reach 90+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "\n",
    "    \n",
    "state_dict = torch.load('model_params.pkl')\n",
    "model = TextRNN()\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "test_demo = ['《时光重返四十二难》恶搞唐增取经一款时下最热门的动画人物：猪猪侠，加上创新的故事背景，震撼的操作快感，成就了这部恶搞新作，现正恶搞上市，玩家们抢先赶快体验快感吧。游戏简介：被时光隧道传送到208年的猪猪侠，必须经历六七四十二难的考验，才能借助柯伊诺尔大钻石的力量，开启时光隧道，重返2008年。在迷糊老师、菲菲公主的帮助下，猪猪侠接受了挑战，开始了这段充满了关心和情谊的旅程。    更多精彩震撼感觉，立即下载该款游戏尽情体验吧。玩家交流才是王道，讯易游戏玩家交流中心 QQ群：6306852-----------------生活要有激情，游戏要玩多彩(多彩游戏)。Colourfulgame (多彩游戏)，让你看看快乐游戏的颜色！精品推荐：1：《钟馗传》大战无头关羽，悲壮的剧情伴随各朝英灵反攻地府！2：《中华群英》将和赵云，项羽，岳飞等猛将作战，穿越各朝代抗击日寇。良品推荐：1：《赌王争霸之斗地主》易飞会在四角恋中会选择谁？是否最终成赌神呢？2：勇者后裔和魔王紧缠一起，前代恩怨《圣火伏魔录》将为您揭示一切。  3：颠覆传统概念，恶搞+非主流？！誓必弄死搞残为止《爆笑飞行棋》。4：《中国象棋残局大师》快棋和人机模式让畅快对弈！一切“多彩游戏”资讯，点击Colourfulgame官网http://www.colourfulgame.com一切“多彩游戏”感言，交流Colourfulgame论坛http://121.33.203.124/forum/【客服邮箱】：xunyiwangluo@126.com\">xunyiwangluo@126.com\">xunyiwangluo@126.com【客服热线】：020-87588437']\n",
    "\n",
    "#for i in test_demo:\n",
    "    #print(i,\":\",model.predict(i))\n",
    "help(model.load_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(5000, 64)\n",
    "        self.rnn = nn.LSTM(input_size=64, hidden_size=128, num_layers=1, bidirectional=False)\n",
    "#         self.f1 = nn.Sequential(nn.Linear(128, 128),\n",
    "#                                 nn.Dropout(0.8),\n",
    "#                                 nn.ReLU())\n",
    "        self.f2 = nn.Sequential(nn.Linear(128, 10),\n",
    "                                nn.Softmax())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x,_ = self.rnn(x)\n",
    "        x = F.dropout(x,p=0.8)\n",
    "        x = self.f2(x[:,-1,:])\n",
    "        #x = self.f2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "model = TextRNN()\n",
    "torch.save(model.state_dict(), './model_params.pkl')\n",
    "    \n",
    "\n",
    "state_dict = torch.load('model_params.pkl')\n",
    "print(state_dict)\n",
    "\n",
    "model_new = TextRNN()\n",
    "model_new.load_state_dict(state_dict)\n",
    "\n",
    "# test_demo = ['《时光重返四十二难》恶搞唐增取经一款时下最热门的动画人物：猪猪侠，加上创新的故事背景，震撼的操作快感，成就了这部恶搞新作，现正恶搞上市，玩家们抢先赶快体验快感吧。游戏简介：被时光隧道传送到208年的猪猪侠，必须经历六七四十二难的考验，才能借助柯伊诺尔大钻石的力量，开启时光隧道，重返2008年。在迷糊老师、菲菲公主的帮助下，猪猪侠接受了挑战，开始了这段充满了关心和情谊的旅程。    更多精彩震撼感觉，立即下载该款游戏尽情体验吧。玩家交流才是王道，讯易游戏玩家交流中心 QQ群：6306852-----------------生活要有激情，游戏要玩多彩(多彩游戏)。Colourfulgame (多彩游戏)，让你看看快乐游戏的颜色！精品推荐：1：《钟馗传》大战无头关羽，悲壮的剧情伴随各朝英灵反攻地府！2：《中华群英》将和赵云，项羽，岳飞等猛将作战，穿越各朝代抗击日寇。良品推荐：1：《赌王争霸之斗地主》易飞会在四角恋中会选择谁？是否最终成赌神呢？2：勇者后裔和魔王紧缠一起，前代恩怨《圣火伏魔录》将为您揭示一切。  3：颠覆传统概念，恶搞+非主流？！誓必弄死搞残为止《爆笑飞行棋》。4：《中国象棋残局大师》快棋和人机模式让畅快对弈！一切“多彩游戏”资讯，点击Colourfulgame官网http://www.colourfulgame.com一切“多彩游戏”感言，交流Colourfulgame论坛http://121.33.203.124/forum/【客服邮箱】：xunyiwangluo@126.com\">xunyiwangluo@126.com\">xunyiwangluo@126.com【客服热线】：020-87588437']\n",
    "\n",
    "# #for i in test_demo:\n",
    "#     #print(i,\":\",model.predict(i))\n",
    "# help(model.load_state_dict)\n",
    "\n",
    "\n",
    "# torch.save(model.state_dict, './model_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
